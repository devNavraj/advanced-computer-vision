{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBhB0ur1J10M"
      },
      "source": [
        "# Fine-Grained Image Classification\n",
        "\n",
        "## Definition on fine-grained image classification\n",
        "The fine-grained image classification task aims to classify images at a sub-category level, in comparison to general image classification task. For example, general image classification aims to identify birds and dogs while fine-grained image classification aims to distinguish different species of birds. This task is a challenging task in computer vision as the visual difference between classes are more difficult to distinguish.\n",
        "\n",
        "### Part 1: Dataset Download\n",
        "\n",
        "- **Task**: Download a fine-grained image dataset for classification task.\n",
        "- **Requirements**:\n",
        "  - Download fine-grained image dataset of FGVC-Aircraft, an aircraft classification dataset. The download webpage is \"https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/\".\n",
        "  - Implement code to read image from the downloaded dataset.\n",
        "  - Visualize sample images from training set and testing set, respectively.\n",
        "  - Score breakdown:\n",
        "    - 2 points: The code can read images from FGVC-Aircraft dataset.\n",
        "    - 2 points: Visualize 10 images from training set and 5 images from testing set.\n",
        "    - 1 point: Discuss what is your observation of the training set?\n",
        "- **What to submit:**\n",
        "  - Submit a1_part1.py file of python code to iLearn Assignment 1 submission. The code should be able to read images from FGVC-Aircraft dataset and can visualize 10 images from training set and 5 images from testing set.\n",
        "  - Your response to the discussion of what is your observation of the training set in below anwser box.\n",
        "\n",
        "### Part 2: Algorithm Selection\n",
        "\n",
        "- **Task**: Select and test at 2 different deep learning methods.\n",
        "- **Requirements**:\n",
        "  - Include one self-designed method and one ResNet-50 model.\n",
        "  - Test ResNet-50 with both fine-tuning and transfer learning.\n",
        "  - Score breakdown:\n",
        "    - 3 points: Include 2 methods with the required models.\n",
        "    - 2 points: Test ResNet-50 with both fine-tuning and transfer learning.\n",
        "- **What to submit:**\n",
        "  - Submit a1_part2_m1.py file of python code to iLearn Assignment 1 submission. The code is to apply ResNet-50 to the classification of FGVC-Aircraft dataset with both fine-tuning setting and transfer learning setting. Testing code is also required.\n",
        "  - Submit a1_part2_m2.py file of python code to iLearn Assignment 1 submission. The code is to apply your self-designed CNN model to the classification of FGVC-Aircraft dataset. Testing code is also required.\n",
        "\n",
        "### Part 3: Performance Improvement\n",
        "\n",
        "- **Task**: Implement strong data augmentation and a learning rate scheduler.\n",
        "- **Requirements**:\n",
        "  - Data augmentation must improve performance.\n",
        "  - Provide a reasonable analysis on the effectiveness of data augmentation.\n",
        "  - Score breakdown:\n",
        "    - 2 points: Data augmentation improves performance.\n",
        "    - 2 points: Implement a learning rate scheduler that improves performance.\n",
        "    - 1 point: Provide a proper analysis on data augmentation.\n",
        "- **What to submit:**\n",
        "  - Submit a1_part3_aug.py file of python code to iLearn Assignment 1 submission. The code is to enable strong data augmentation to your designed model. Testing code is also required. An improved classification accuracy is expected.\n",
        "  - Submit a1_part3_lr.py file of python code to iLearn Assignment 1 submission. The code is to enable an improved learning rate to your designed model. Testing code is also required. An improved classification accuracy is expected.\n",
        "  - Your response of reasonable analysis on the effectiveness of data augmentation in below anwser box.\n",
        "\n",
        "### Part 4: Deeper Analysis\n",
        "\n",
        "- **Task**: Analyze limitations of the dataset and algorithms, propose improvements.\n",
        "- **Requirements**:\n",
        "  - Identify dataset limitations.\n",
        "  - Discuss limitations of the 2 algorithms.\n",
        "  - Apply GAN and Deepdream for augmentation and anaylysis.\n",
        "  - Score breakdown:\n",
        "    - 1 point: Identify dataset limitations.\n",
        "    - 1 point: Discuss limitations of the 2 algorithms.\n",
        "    - 1 point: Apply GAN to generate 10 sample images of any one class in FGVC-Aircraft dataset.\n",
        "    - 2 points: Apply Deepdream algorithm and show 3 resulting images.\n",
        "- **What to submit:**\n",
        "  - Submit a1_part4_GAN.py file of python code to iLearn Assignment 1 submission. The code is to apply GAN to generate 10 samples based on any one of the categories from FGVC-Aircraft dataset. The 10 sample images visualization is required in the code.\n",
        "  - Submit a1_part4_Deep.py file of python code to iLearn Assignment 1 submission. The code is to apply Deepdream to generate 3 resulting images. The visualization of resulting images is required in the code.\n",
        "  - Your response to dataset limitation in below anwser box.\n",
        "  - Your response to limitation of the two methods in below anwser box.\n",
        "  \n",
        "## Evaluation Criteria\n",
        "\n",
        "Your assignment will be evaluated based on the following criteria:\n",
        "\n",
        "- Fulfillment of assignment requirements.\n",
        "- Design and testing of deep learning methods.\n",
        "- Implementation of performance improvement techniques.\n",
        "- Depth of analysis and proposed improvements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rgv0ohwp4XH"
      },
      "source": [
        "### Part 1: Dataset Downloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHScAA3wq_Td"
      },
      "source": [
        "Discuss your observations from the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Most aircraft classes are visually very similar, often varying only by manufacturer, length, engine configuration, or tail design. This makes the problem harder than generic object classification. \n",
        "- It contains a wide spread of aircrafts - large twins like the 737‑900 and A330‑200 to regional jets (ERJ‑145, BAE‑125) and business/turboprops (Gulfstream IV, EMB‑120)—so our model will learn very fine shape and panel‑line distinctions.  \n",
        "- Image samples are from different lighting, angles, liveries, and backgrounds (airfield, urban, grass, hangar, sky).This helps the model generalize better if sufficient examples per class are present.\n",
        "- Images are clear and high resolution (approx. 1–2 MP as per FGVC docs). Hence, they are good for CNNs. \n",
        "- The images appear to have been preprocessed correctly: no bottom copyright banners are visible in samples, indicating `remove_banner=True` was active, which is a necessary preprocessing steps mentioned in FGVC documentation.\n",
        "- Aircrafts are consistently in the center of the frame and mostly horizontally aligned which is again favorable for CNNs.\n",
        "- The copyright banner‑removal logic and 224×224 resizing give consistency, yet some images still look soft or overcast—adding brightness/contrast jitter or a light sharpening step can help cover varied lighting conditions.  \n",
        "- Simple augmentations (random flips, small rotations, color jitter, slight random crops) will further encourage the model to generalize across different poses and atmospheres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWMjQ7xKJ10d"
      },
      "source": [
        "### Part 3: Performance Improvement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o6zH1_nJ10d"
      },
      "source": [
        "Answer to data augmentation analysis and accuracy:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Observed Change**:\n",
        "- **Before** (Baseline CNN or shallow model): ~14% accuracy\n",
        "- **After** (Same model + strong augmentation): ~24% accuracy  \n",
        "- **~71% relative improvement in accuracy**  \n",
        "This improvement demonstrates that **my model started generalizing better**, not just memorizing training samples.\n",
        "\n",
        "The original 14% accuracy likely came from **overfitting to small, local patterns** (e.g., logo shape, sky background). With augmentations, your model sees **a richer version of the dataset** during every epoch. It is now **forced to generalize** to shape, silhouette, tail-fin, window pattern, and engine placement rather than color schemes or logo alignment.\n",
        "\n",
        "### **Breakdown of Each Augmentation's Contribution**\n",
        "\n",
        "**1. `RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.9, 1.1))`**\n",
        "- **Effect**: Forces model to learn from different spatial parts of the aircraft.\n",
        "- **Why it helps**: Aircraft photos might contain logos, engines, wings, or windows at varying scales—this helps model learn features that are **not dependent on position** or size.\n",
        "\n",
        "**2. `RandomHorizontalFlip()`**\n",
        "- **Effect**: Inverts image left-right randomly.\n",
        "- **Why it helps**: Aircraft from either side might appear in images. Horizontal flip forces invariance to orientation.\n",
        "\n",
        "**3. `ColorJitter()`**\n",
        "- **Effect**: Randomly changes brightness, contrast, saturation, and hue.\n",
        "- **Why it helps**: Aircraft are photographed under different lighting conditions (sunny, overcast, dusk). This avoids overfitting to color tone and enhances **illumination invariance**.\n",
        "\n",
        "**4. `RandomRotation(15)`**\n",
        "- **Effect**: Slight rotation in ±15° range.\n",
        "- **Why it helps**: Viewpoint may vary (especially during takeoff/landing). Rotation makes model robust to mild angular displacements.\n",
        "\n",
        "**5. `RandomPerspective(distortion_scale=0.3)`**\n",
        "- **Effect**: Simulates 3D viewpoint distortion by shifting corner points.\n",
        "- **Why it helps**: Introduces **perspective variation** resembling oblique viewpoints—common in aircraft photography.\n",
        "\n",
        "**7. `RandomErasing(p=0.5, scale=(0.02, 0.2))`**\n",
        "- **Effect**: Randomly masks a rectangular region in the image.\n",
        "- **Why it helps**: Prevents reliance on single parts (e.g., airline logo), encourages **holistic understanding** of the aircraft structure.\n",
        "\n",
        "Such **strong data augmentations** introduce controlled chaos, helping my model:\n",
        "- Pay attention to shape, proportion, component placement\n",
        "- Avoid overfitting to easily memorized visual cues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6xhiX76J10f"
      },
      "source": [
        "### Part 4: Deeper Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-erFwer2G1"
      },
      "source": [
        "\n",
        "  - Your response to dataset limitation in below anwser box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The FGVC-Aircraft benchmark dataset contains only about $10,200$ images (roughly 100 images for each of 102 aircraft variants)​. This relatively small size is a fundamental limitation – deep models risk overfitting when training data per class is so limited​.\n",
        "- Additionally, the images exhibit high intra-class variability (e.g. different angles, lighting, and backgrounds for the same aircraft type) and low inter-class variability (many aircraft variants look extremely similar)​. Such subtle visual differences between classes make it challenging for models to learn discriminative features. \n",
        "- Moreover, the dataset images are contributed by aircraft spotters and often include cluttered backgrounds (airport tarmacs, skies, other aircraft) that introduce noise​.\n",
        "- While labels are organized in a hierarchy (model variant, family, manufacturer), certain variants are nearly indistinguishable, and the labeling had to collapse those cases​.\n",
        "- Annotation consistency is generally high for class labels, but the fine-grained nature means even minor labeling errors or ambiguities (e.g. misidentifying a sub-variant) can hurt performance. \n",
        "- In summary, the dataset’s limited size, biased image conditions, and fine granularity of classes all impose challenges. The result is that models can easily overfit to spurious details or background cues rather than true aircraft-specific features, underscoring the need for data augmentation and careful model design for this domain​."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsXQ5fnaKuHd"
      },
      "source": [
        "\n",
        "  - Your response to limitation of the two methods in below anwser box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Limitation of ResNet-50 Pre-Trained Model**\n",
        "\n",
        "- Using a generic **ResNet-50** pre-trained on **ImageNet** as a starting point provides robust general features, but it has limitations for fine-grained aircraft recognition. There is a domain gap – ImageNet pre-training teaches the network to recognize broad object classes, not the subtle shape differences between, say, a Boeing 737-700 and 737-800. \n",
        "- Without significant **fine-tuning**, the ResNet’s filters may not optimally discriminate fine-grained details (e.g. engine placement or tail shape). \n",
        "- Another issue is **overfitting**: with only a few thousand training images, a high-capacity model like **ResNet-50** can memorize training quirks if fine-tuned too aggressively. The network might latch onto background-color correlations or airline liveries unique to the training set rather than true variant features. \n",
        "- Indeed, FGVC tasks often report overfitting when large networks are trained on small datasets​. **Regularization** and **early stopping** are needed to avoid overfitting. \n",
        "- A related limitation is **limited interpretability** – ResNet-50 is a complex 50-layer architecture, so understanding which features it uses for decisions is difficult. This complicates debugging when it confuses look-alike aircraft. \n",
        "- Lastly, ResNet’s fixed input size (often $224×224$) forces heavy image resizing; fine details like cockpit window shape or logo might be lost. \n",
        "- In summary, while **ResNet-50** provides a strong baseline, it requires careful adaptation to avoid missing fine details or focusing on the wrong cues for fine-grained aircraft classification.\n",
        "\n",
        "**Limitation of Self-Designed CNN Model**\n",
        "\n",
        "- A **custom-designed CNN** (built from scratch for this task) gives full control over architecture, but it typically has far fewer layers/parameters than sophisticated pre-trained models like ResNet-50 and no pre-trained knowledge. This leads to several limitations. First, a smaller CNN may have **limited capacity to capture** the subtle differences between many aircraft types. \n",
        "- If the network is too shallow, it might not develop the high-level discriminative features needed. Conversely, if it is made deep without pre-training, the lack of data can cause severe **overfitting**. \n",
        "- Training from scratch on ~6,600 training images (for 100 classes) is data-starved – the model may memorize training examples rather than generalize, since it doesn’t benefit from millions of images of pre-training. This means the custom CNN might converge to a lower accuracy ceiling than a fine-tuned ResNet. \n",
        "- Additionally, without advanced components (like residual connections or attention modules), a plain CNN might struggle with the saliency of fine parts – e.g. focusing on an aircraft’s distinctive engines or tailfin. It may end up using background or overall color, which are not reliable identifiers (many airliners are white!). \n",
        "- The **training efficiency is also lower**: it takes longer for a scratch model to learn basic visual patterns (edges, textures) that a pre-trained model already “knows.” \n",
        "- In practice, one might **need extensive data augmentation** and possibly **synthetic data** such as using **GAN** or **DeepDream** to make a self-designed CNN perform well. \n",
        "- In summary, a self-designed CNN faces capacity vs. overfitting trade-offs and typically underperforms a well-tuned transfer learning approach on this fine-grained image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "devg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
